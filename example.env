# CARLA Configuration
CARLA_EXE=

# Parallel Training Configuration
N_ENVS=2
BASE_PORT=2000
PORT_STRIDE=2
SEED=1
RESUME_CHECKPOINT=

# Training Configuration
RUN_FPS=25
SUBSTEPS_PER_STEP=5
MODEL_SAVE_FREQ=50000
VIDEO_SAVE_FREQ=5000
TIME_LIMIT=3000
PROJECT_NAME=
RUN_NAME=
ENABLE_RENDERING=false

# ROAR Berkeley Style Reward Configuration (adapted to continuous progress)
# Based on: https://roar.berkeley.edu/roar-end-to-end-reinforcement-learning

# Progress reward: continuous reward for forward distance (scaled to match Berkeley's 15 per checkpoint)
PROGRESS_SCALE=15.0

# Step penalty ("hot water"): constant negative pressure to encourage speed (scaled for 25 FPS)
STEP_PENALTY=0.4

# Collision: terminate + explicit penalty
COLLISION_THRESHOLD=1.0
COLLISION_PENALTY=25.0

# Stalling penalty: penalize being stuck (speed < 1 m/s)
STALL_FRAMES_THRESHOLD=10
STALL_PENALTY=25.0

# Reverse penalty: penalize going backward
REVERSE_PENALTY=25.0

# Steering deadzone: reward for stable steering (reduces wobbling)
STEERING_DEADZONE=0.01
STEERING_DEADZONE_REWARD=0.1

# Heading penalty: weak penalty to provide steering gradient (allows racing lines)
HEADING_PENALTY_SCALE=0.1
HEADING_PENALTY_THRESHOLD=0.4
HEADING_LOOKAHEAD=10.0

# Observation Configuration
NUM_LIDAR_BEAMS=60
LIDAR_MAX_DISTANCE=200.0

# SAC Hyperparameters (hardcoded in train_online.py, documented here for reference)
# LEARNING_RATE=1e-4
# BATCH_SIZE=512
# GAMMA=0.995
# ENT_COEF=0.2
# GRADIENT_STEPS=4
# LEARNING_STARTS=10000
# NET_ARCH=[512, 512, 256]
